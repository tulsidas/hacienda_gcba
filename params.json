{"name":"Hacienda gcba","tagline":"experimento para parsear los datos de hacienda de GCBA","body":"hacienda gcba\r\n=============\r\n\r\nqué y por qué\r\n-------------\r\n\r\nUn experimento para parsear los datos de hacienda de GCBA, en particular los que respectan a compras y licitaciones.\r\n\r\nEn un mundo (o país, o sistema de gobierno) ideal, la transparencia sobre el gasto público sería completa, y podríamos (los ciudadanos) saber exactamente cuánto se gastó y en qué.\r\n\r\nEl Gobierno de la Ciudad de Buenos Aires publica en [su sitio web] [1] un buscador con las licitaciones adjudicadas, pero los datos no son abiertos y la interfaz no permite mucho que digamos.\r\n\r\nEl propósito de este proyecto es parsear la información, procesarla y publicarla libremente.\r\n\r\ncómo\r\n----\r\n\r\nEl trabajo consistió principalmente en estos pasos \r\n\r\n1. Bajar el listado de licitaciones\r\n2. Parsear datos generales de cada licitación\r\n3. Bajar y parsear detalles de las licitaciones\r\n4. Limpiar la base de datos\r\n5. Exportar la base de datos como un archivo .csv\r\n6. Refinar la información\r\n7. Publicarla (en proceso)\r\n\r\nDetallando:\r\n\r\n#### Paso 1: Bajar el listado de licitaciones\r\n[El sitio de hacienda] [1] tiene un buscador que muestra de a 100 resultados máximo. \r\nPor suerte el buscador usa **GET** lo cual expone el número de página que se está accediendo como parámetro web, \r\npor lo que el primer paso consistió en escribir un pequeño script en [bash] [2] que baje las 460 páginas (con 100 resultados cada una) listando las ~46.000 licitaciones adjudicadas. \r\n\r\n#### Paso 2: Parsear datos generales de cada licitación\r\nEl segundo paso consistió en tomar la información bajada y parsearla (procesarla) para extraer la infomación de la licitaciones propiamente dichas. El problema es que lo bajado en el paso 1 esta en formato **html**, lo cual bastante molesto para procesar, pero con la imprescindible ayuda de la librería [jsoup] [3] el asunto fue bastante sencillo.\r\n\r\n#### Paso 3: Bajar y parsear detalles de las licitaciones\r\nUn detalle no menor, es que en el listado bajado en el paso 1 había información sobre la licitación (quién la solicitaba, fecha, etc) pero el resto de la información (notablemente qué empresa fue la adjudicada) estaba en *otro html*, por lo que el paso 3 consistió en bajar los ~46.000 archivos de detalle y parsearlos para obtener la información complementaria \r\n\r\n#### Paso 4: Limpiar la base de datos\r\nEste paso fue un mero tecnicismo, ya que el link de donde se encuentra el archivo de la adjudicacion varía según licitación, en algunos casos es pdf, en otros zip, en otros doc, etc., el problema consistió en que algunos links eran absolutos (o sea incluían http://www.buenosaires.gob.ar) y otros no, el paso 3 asumía que eran todos relativos y agregaba de prepo http://www.buenosaires.gob.ar, por lo que hubo que limpiar los que contenían el dominio 2 veces\r\n\r\n#### Paso 5: Exportar la base de datos como un archivo .csv\r\nHasta ahora toda la información era guardada en una [base de datos embebida] [4], para prepararla para el paso 6 hubo que transformar los ~46k registros en un [archivo separado por comas] [5]\r\n\r\n#### Paso 6: Refinar la información\r\nEl paso más importante (y más difícil) de todos. Tener los registros no alcanza, son muchos, y es muy largo y tedioso intentar hacer cualquier análisis sobre él.\r\nEn este paso se usó la excelente herramienta [OpenRefine] [6], que permite, entre muchísimas otras cosas, limpiar información \"sucia\".\r\n\r\nConcretamente, la mayor limpieza fue dada en las empresas. Por ejemplo, algunos registros indicaban que la empresa adjudicada era *Ernesto Van Rossum y Cía S.R.L.*, mientras que otros indicaban *Ernesto Van Rossum y CIA SRL* o *Ernesto Van Rossum y Cía SRL*\r\n\r\nA primera vista parece que no hay diferencias entre los 3 casos, pero viendo de cerca en un caso dice **S.R.L.** y en otro **SRL**, y otras diferencias sutiles. Esto es casi imperceptible para una persona, pero una computadora trata los 3 items como si fueran 3 distintos, y si quisieramos saber cuántas adjudicaciones tuvo una empresa, no incluiría en los resultados aquellas que no tienen exactamente el mismo nombre.\r\n\r\nPor suerte [OpenRefine] [6] detecta estas diferencias sutiles y permite agruparlos bajo el mismo nombre, de forma que sea fácil la agrupación o filtrado.\r\n\r\nEste proceso se realizó tanto para empresas como para solicitantes, licitantes y rubros.\r\n\r\n#### Paso 7: Open Data\r\nEl paso 7 (aún en proceso) consta(rá) de publicar la información, sea como csv descargable, tanto como proveer un API para aquellos programadores/hackers que quieran usarla y también, sobre todo, una plataforma de visualización que permita ver las licitaciones y, eventualmente, detectar irregularidades.\r\n\r\nqué sigue\r\n---------\r\n\r\nMás allá del paso 7, hay varias cosas por hacer.\r\n\r\nEl primero y principal es que las licitaciones no tienen, al momento, costo asociado. Con lo hecho actualmente no hay forma fácil de saber si la licitación fue por pocos pesos o por varios millones. Esta información está guardada en los archivos adjuntos de las licitaciones, que por un lado son muchos y muy pesados, y por el otro están en formatos muy disímiles (pdf, word, rtf, zips) lo cual hace muy difícl el procesamiento automático\r\n\r\nPor otro lado, no hay un agrupamiento a nivel rubro. Estaría bueno poder ver cuánto se gasta a nivel *salud*, por ejemplo, pero los rubros indican cosas como *Compra de material estéril* y de nuevo, a priori, no hay forma facil de asociar los rubros que están escritos en las licitaciones entre sí.\r\n\r\ncontacto\r\n--------\r\n\r\nDudas, consultas, sugerencias o cualquier otro menester: tulsidas arroba gmail.com o twitter: @quixote_arg\r\n\r\n[1]: http://www.buenosaires.gob.ar/areas/hacienda/compras/        \"Hacienda GCBA\"\r\n[2]: http://en.wikipedia.org/wiki/Bash_(Unix_shell)\r\n[3]: http://jsoup.org/\r\n[4]: http://hsqldb.org/\r\n[5]: http://es.wikipedia.org/wiki/CSV\r\n[6]: http://openrefine.org/\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}